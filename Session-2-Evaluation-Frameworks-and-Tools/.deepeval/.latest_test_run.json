{"testRunData": {"testCases": [{"name": "test_case_0", "input": "\nAlice: \"Today's agenda: product update, blockers, and marketing timeline. Bob, updates?\"\nBob: \"Core features are done, but we're optimizing performance for large datasets. Fixes by Friday, testing next week.\"\nAlice: \"Charlie, does this timeline work for marketing?\"\nCharlie: \"We need finalized messaging by Monday.\"\nAlice: \"Bob, can we provide a stable version by then?\"\nBob: \"Yes, we'll share an early build.\"\nCharlie: \"Great, we'll start preparing assets.\"\nAlice: \"Plan: fixes by Friday, marketing prep Monday, sync next Wednesday. Thanks, everyone!\"\n", "actualOutput": "\nIntro:\nAlice outlined the agenda: product updates, blockers, and marketing alignment.\n\nBody:\nBob reported performance issues being optimized, with fixes expected by Friday. Charlie requested finalized messaging by Monday for marketing preparation. Bob confirmed an early stable build would be ready.\n\nConclusion:\nThe team aligned on next steps: engineering finalizing fixes, marketing preparing content, and a follow-up sync scheduled for Wednesday.\n", "success": true, "metricsData": [{"name": "Format Correctness [GEval]", "threshold": 0.5, "success": true, "score": 1.0, "reason": "The actual output contains all required headings: 'intro', 'body', and 'conclusion', and they are presented in the correct order. Each section is clearly labeled and the content under each heading is relevant and complete, fully aligning with the evaluation steps.", "strictMode": false, "evaluationModel": "gpt-4.1", "evaluationCost": 0.001272, "verboseLogs": "Criteria:\nNone \n \nEvaluation Steps:\n[\n    \"The `actual_output` is completely wrong if it misses any of the headings: 'intro', 'body', 'conclusion'.\",\n    \"If the `actual_output` has all the complete headings but are in the wrong order, penalize it.\",\n    \"If the summary has all the correct headings and they are in the right order, give it a perfect score.\"\n] \n \nRubric:\nNone \n \nScore: 1.0"}], "runDuration": 1.5991767650120892, "evaluationCost": 0.001272, "order": 0}], "conversationalTestCases": [], "metricsScores": [{"metric": "Format Correctness [GEval]", "scores": [1.0], "passes": 1, "fails": 0, "errors": 0}], "prompts": [], "testPassed": 1, "testFailed": 0, "runDuration": 1.6411966309824493, "evaluationCost": 0.001272}}